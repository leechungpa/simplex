defaults:
  - _self_
  - augmentations: imagenet.yaml
  - wandb: private.yaml
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

hydra:
  output_subdir: null
  run:
    dir: .

###############
# job name
name: ""   # experiment name
name_kwargs:
  add_method: True   # Include the training method in the experiment name
  add_batch_size: True   # Include the batch size in the experiment name
  add_k: False   # Exclude parameter `k` from the experiment name
  add_p: False   # Exclude parameter `p` from the experiment name
  add_lamb: True   # Include lambda (`lamb`) in the experiment name
  add_temperature: True   # Include temperature in the experiment name

###############
# loss
method: ""   # Self-supervised learning method to use (e.g., SimCLR, DCL, etc.)
method_kwargs:
  proj_hidden_dim: 2048
  proj_output_dim: 128
  temperature: 0.3  # Temperature parameter

###############
add_vrn_loss_term:
  enabled: True
  weight: 40   # Weight assigned to the variance regularization term
  p: 2
  k: 130000

###############
# model and optimizer
backbone:
  name: "resnet50"  # --- backbone architecture ---
optimizer:
  name: "sgd" # "sgd", "lars", "adam", "adamw"
  batch_size: 128 # --- batch size options: 32, 64, 128, 256, 512 ---
  lr: 0.1  
  lr_method: "linear"   # Learning rate scaling method: "linear" or "square_root"
  weight_decay: 1e-4
  classifier_lr: 0.1
scheduler:
  name: "warmup_cosine"   # Learning rate scheduler
  warmup_epochs: 10
max_epochs: 200  # --- Total number of training epochs: 200, 500, 1000 ---

###############
# data
data:
  dataset: imagenet100
  train_path: # --- path to training data ---
  val_path: # --- path to validation data ---
  format: "image_folder"
  num_workers: 4

###############
# evaluation in batch
evaluate_batch:
 enable: False
 type: "all"
 skip_before_optm: False

###############
# save and resume
checkpoint:
  enabled: True
  dir: # --- path to save checkpoints ---
  frequency: 50
auto_resume:
  enabled: False

###############
# gpu options
devices: [0]   # --- specify GPU devices ---
sync_batchnorm: True
accelerator: "gpu"
strategy: "ddp"
precision: 16-mixed