defaults:
  - _self_
  - augmentations: cifar.yaml
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

hydra:
  output_subdir: null
  run:
    dir: .

wandb:
  enabled: True
  entity: null
  project: vrn_cifar
  offline: false

###############
# job name
name: ""
name_kwargs:
  add_method: False
  add_batch_size: False
  # For simplex loss
  add_k: False
  add_p: False
  add_lamb: False
  # For simclr loss
  add_temperature: False

###############
# loss
method: ##########
method_kwargs:
  proj_hidden_dim: 512
  proj_output_dim: 128
  temperature: ##########

###############
# add additional simplex loss
add_simplex_loss:
  enabled: True
  weight: ##########
  p: 2
  k: 50000
  rectify_large_neg_sim: False
  rectify_small_neg_sim: False
  disable_positive_term: True

###############
# model and optimizer
backbone:
  name: "resnet18"
optimizer:
  name: "sgd" # "sgd", "lars", "adam", "adamw"
  batch_size: ##########
  lr: 0.3  # Reference: Bridging Mini-Batch and Asymptotic Analysis in Contrastive Learning: From InfoNCE to Kernel-Based Losses
  lr_method: "linear"   # "square_root", "linear"
  # weight_decay: 0
  weight_decay: 1e-4
  classifier_lr: 0.1
scheduler:
  name: "warmup_cosine"
  warmup_epochs: 10
  # warmup_epochs: 0
max_epochs: ##########

###############
# data
data:
  dataset: cifar100 # cifar10, cifar100
  train_path: /data/files/torchvision
  val_path: /data/files/torchvision
  format: "image_folder"
  num_workers: 4

###############
# evaluation in batch
evaluate_batch:
 enable: False
 type: "all"
 skip_before_optm: False

###############
# save and resume
checkpoint:
  enabled: True
  dir: /data/cl/simplex/final_models
  frequency: 50
auto_resume:
  enabled: False

###############
# gpu options
devices: [0]
sync_batchnorm: True
accelerator: "gpu"
strategy: "ddp"
precision: 16-mixed