defaults:
  - _self_
  - augmentations: # --- your augmentation config file ---
  - wandb: # --- your WandB config file ---
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

hydra:
  output_subdir: null
  run:
    dir: .

###############
# job name
name: # --- job name ---
name_kwargs:
  add_method: True
  add_batch_size: True
  # For simplex loss
  add_k: False
  add_p: False
  add_lamb: False
  # For simlcr loss
  add_temperature: False

seed: 1234

###############
# loss
method: # Select the loss function: "simplex", "simclr", "barlow_twins"
method_kwargs:
  proj_hidden_dim: 2048 # For Imagenet (For CIFAR:512)
  proj_output_dim: 2048 # For Imagenet (For CIFAR:128)
  # For simplex
  p: 2
  k: 50000 # For CIFAR
  lamb: 1.0
  unimodal: False # Confirm by Sehee.
  rectify_large_neg_sim: False
  rectify_small_neg_sim: False
  # For simclr
  temperature: 0.1
  # For barlow_twins
  scale_loss: 0.1

###############
# add additional simplex loss
add_simplex_loss:
  enabled: True
  weight: 1.0
  p: 2
  k: 50000 # For CIFAR
  lamb: 1.0
  unimodal: False # Confirm by Sehee.
  rectify_large_neg_sim: False
  rectify_small_neg_sim: False

###############
# model and optimizer
backbone:
  name: # --- backbone architecture ---
optimizer:
  name:  # Select the optimizer: "sgd", "lars", "adam", "adamw"
  batch_size: 64 # adjust based on your experiment (32, 64, 128, 256 512)
  lr: 0.075
  lr_method: "square_root" # "square_root"
  weight_decay: 1e-6
  classifier_lr: 0.1
scheduler:
  name: "warmup_cosine"
max_epochs: 1000 # adjust based on your experiment (200 400 1000)

###############
# data
data:
  dataset: cifar100
  train_path: # --- path to your training data ---
  val_path: # --- path to your validation data ---
  format: "image_folder"
  num_workers: 2

###############
# evaluation in batch
evaluate_batch:
 enable: True
 type: "all"
 skip_before_optm: False

###############
# save and resume
checkpoint:
  enabled: True
  dir: # --- directory to save checkpoints ---
  frequency: 50
auto_resume:
  enabled: False

###############
# gpu options
devices: [0]
sync_batchnorm: True
accelerator: "gpu"
strategy: "ddp"
precision: 32-true